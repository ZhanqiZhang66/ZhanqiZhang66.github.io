---
layout: page
title: "Hi, I'm Victoria"
subtitle: "Researcher in Artificial and Biological Intelligence. Bird Lover."
css: "/assets/css/index.css"
share-img: /assets/img/victoria.jpg
use-site-title: true
cover-img:
  - "/assets/img/big-imgs/balboa.JPEG" : "Balboa Park, San Diego (2022)"
  - "/assets/img/big-imgs/bird.jpeg" : "Forest Park, Saint Louis, MO (2021)"
  - "/assets/img/big-imgs/cherry.JPG" : "WashU, Saint Louis, MO (2020)"
  - "/assets/img/big-imgs/child.JPG" : "Jinan, China (2000)"
  - "/assets/img/big-imgs/child_2.JPG" : "Jinan, China (2002)"
  - "/assets/img/big-imgs/dog_2.JPG" : "Creve Coeur Lake. Saint Louis, MO (2018)"
  - "/assets/img/big-imgs/grad_fountain.jpeg" : "WashU Brookings Quadrangle, Saint Louis, MO (2020)"
  - "/assets/img/big-imgs/seaside.JPG" : "Sunset Cliffs, La Jolla, CA (2021)"
  - "/assets/img/big-imgs/yosemite_2.JPEG" : "Yosemite Valley, Yosemite National Park, CA (2022)"
  - "/assets/img/big-imgs/river.jpg" : "Jinan, China (2018)"
  - "/assets/img/big-imgs/freshman.JPG" : "WashU, Saint Louis, MO (2016)"
  - "/assets/img/big-imgs/sea.jpeg" : "La Jolla tide pool, La Jolla, CA (2022)"
  - "/assets/img/big-imgs/ironmountain.JPG" : "Iron Mountain Trail, Poway, CA (2023)"
  - "/assets/img/big-imgs/ski.jpeg" : "Palisade, Tahoe, CA (2023)"
  - "/assets/img/big-imgs/flower.JPG" : "La Jolla Cove, La Jolla, CA (2021)"
---

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

<h3>About me</h3>
<p>
 
  Hii! My name is Victoria Zhanqi Zhang, 张展旗. I am a Ph.D. student in Computer Science at UCSD, supported by HDSI Phd Fellowship. I am honored to be co-advised by Dr. Mikio Aoi and Dr. Gal Mishne. Previously, I studied Computer Science and Electrical Engineering, and advised by Dr. Carlos Ponce at Washington University in St. Louis. I was the Co-president of <a href="https://www.tbp.org/off/DisplayChapterInfo.cfm?ID=99">Tau Beta Pi Engineering Honor Society</a>, Missouri Gamma Chapter. 
</p>
<p>
  I am broadly interested in how machine and biological intelligence complement each other to push forward the discoveries of the brain. My research interests include machine learning, computational system neuroscience, optimization and signal processing. Specifically, I am currently interested in how human behavior reflects complex neural and physiological processes such as learning, perception, and decision-making in the brain. Using tools in machine learning, computational neuroscience, optimization, and signal processing, I attempt to construct unsupervised animal and human action recognition and behavior classification models to understand disorders in clinical settings. 
</p>
<p>
  I’m documenting my learning notes in this <a href="https://zhanqizhang66.github.io/blog/">blog</a>.
</p>
<p>
  When not in the lab, I enjoy <a href="https://zhanqizhang66.github.io/art/">water-color painting</a>, hiking, surfing and teaching my cockatiel Ashe singing and cool tricks.
</p>



<h3>Research</h3>
<ul>
  <! –– 
   <li>
    <p><a href="http://mishne.ucsd.edu/">Mishne Lab</a> and <a href="https://aoilab.biosci.ucsd.edu/">Aoi Lab</a> co-advised by Dr. Gal Mishne and Dr. Mikio Aoi in Department of Data Science, Computer Science, Neuroscience at University of California, San Diego</p>
    <p>This interdisciplinary research project aims to build an unsupervised framework to address a need for quantification of undirected human behavior in open-field clinical assessment and analysis. It will combine computer vision, deep learning, and probabilistic reasoning to extract spatiotemporal dynamics of human subjects from video. Furthermore, we will differentiate behavioral motifs and quantify hallmark features related to bipolar disorder. The principles of our approach are not specific to certain tasks or animal models. In the future, we plan to extend our framework to study and compare behavioral structure in other neuropsychiatric disorders, within or across species. Furthermore, by combining our methods with neural activity dynamics, we could further associate behaviors to neural recordings to gain more insights on how information processing maps onto behaviors, putting us one step closer to understanding the link between the brain and the mind.
    </p>
    </li>
  <li><p><a href="https://ponce.hms.harvard.edu/">Ponce Lab</a> advised by Dr. Carlos Ponce in Department of Neurobiology at Harvard Medical School</p><p>Automated visual recognition has the potential to change many facets of society, from biomedical imaging to security and transportation. Convolutional neural networks (CNNs) are the best models for visual recognition, and while they show enormous promise, they are notably vulnerable to "black-box attacks" -- malicious inputs designed to make the networks make mistakes. Because CNNs share many properties with the brain, we can understand what kind of attacks are particularly effective on the most resilient CNNs, by crafting attacks that manipulate activity in the brain. The Ponce lab has shown that it is possible to use generative adversarial networks (GANs) to maximize the activity of individual neurons in the brain, through the synthesis of artificial images. I will explore which types of GANs are best in achieving this goal, in both macaque brains and convolutional neural networks.</p></li>
  <li><p><a href="http://www.clinicalpharmstl.org/research/Researcher%20Profile%20-%20Bhooma%20Aravamutha,%20M.D.,%20DPhil.html">Aravamuthan Lab</a> advised by Dr. Bhooma Aravamuthan in Department of Neurology in Washington University in St. Louis School of Medicine</p><p> I am developing an open-field, video-based animal pose tracking framework using computer vision. </p>
  <li><p><a href="https://aimlab.wustl.edu/">AIM Lab</a> advised by Dr. Shantanu Chakrabartty in Department of Electrical Engineering in Washington University in St. Louis. </p><p>I investigated sonification techinique that can be used to visualize high-dimensional data like images. The goal is to understand the benefits of sonification compound to visual representations especially in the context of human-in-the-loop systems</p></li>
</ul>
