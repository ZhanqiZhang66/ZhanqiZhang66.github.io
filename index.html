---
layout: page
title: "Hi, I'm Victoria"
subtitle: "Decoding intelligence — in brains, behavior, and machines." # in Artificial and Biological Intelligence
share-img: /assets/img/victoria_22.jpg
use-site-title: true
cover-img:
  - "/assets/img/bird/b_k_4.jpeg": "Blueberry and Kiwi"
---

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-Y06S3E3WTE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-Y06S3E3WTE');
</script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

<h3>News</h3>
<ul>
  <li><strong>Hire Me</strong> — Open to full-time Research Scientist roles in 2026. Connect with me on <a href="https://www.linkedin.com/in/zhanqi-zhang-9873cle/">LinkedIn</a>!</li>
  <li>My submission, <em>BEHAVE: Behavioral Ethology for Human Assessment via Variational Encoding</em>, has been accepted at the <a href="https://data-brain-mind.github.io/">NeurIPS 2025 Workshop on Data on Brain and Mind (DBM)</a>.</li>
  <li>My paper, <a href="https://www.science.org/doi/10.1126/sciadv.adq7342">Brain Feature Maps Reveal Progressive Animal-Feature Representations</a>, is published in <em>Science Advances</em> and featured as “<a href="https://brain.harvard.edu/hbi_news/when-neurons-discover-on-their-own/">When Neurons Discover on Their Own</a>” by Harvard Brain Institute News.</li>
  <li>Check out my preprint on <a href="https://www.medrxiv.org/content/10.1101/2024.11.14.24317348v2">Behavioral Dynamics in Bipolar Disorder</a> on medRxiv.</li>
  <li>Co-authored paper <a href="https://www.nature.com/articles/s41586-025-09544-4">Arousal as a universal embedding for spatiotemporal brain dynamics</a> now published in <em>Nature</em>.</li>
</ul>

<h3>About Me</h3>

<p class="about-text">
<span class="fas fa-graduation-cap about-icon"></span>
Hi! I’m <strong>Victoria Zhanqi Zhang (张展旗)</strong>, a Ph.D. candidate in Computer Science at <a href="https://cse.ucsd.edu/">UC San Diego</a>, supported by the HDSI Ph.D. Fellowship. I’m co-advised by <a href="https://aoilab.biosci.ucsd.edu/">Dr. Mikio Aoi</a> and <a href="http://mishne.ucsd.edu/">Dr. Gal Mishne</a>. I earned dual degrees in Computer Science and Electrical Engineering from <a href="https://engineering.wustl.edu/">Washington University in St. Louis</a>, where I worked with <a href="https://ponce.hms.harvard.edu/">Dr. Carlos Ponce</a> as an undergraduate researcher.
</p>

<p class="about-text">
<span class="fas fa-code about-icon"></span>
I develop machine learning models to uncover structure and dynamics in complex behavioral and brain systems. My research bridges computational neuroscience and deep learning to understand how natural learning in the brain can inspire more robust, interpretable AI.
</p>

<p><strong>Computational Neuroscience</strong> — I study how neural representations and learning dynamics emerge across brain hierarchies using electrophysiological decoding, hierarchical modeling, and behavioral analysis. In <em><a href="https://www.science.org/doi/10.1126/sciadv.adq7342">Brain Feature Maps Reveal Progressive Animal-Feature Representations</a></em>, I showed how hierarchical representations of animal features arise along the primate ventral visual stream. I also develop behavioral models to identify state-dependent dynamics in psychiatric disorders such as bipolar disorder (<a href="https://www.medrxiv.org/content/10.1101/2024.11.14.24317348v1">preprint</a>).</p>

<p><strong>Machine Learning</strong> — I build interpretable and robust models that learn from complex neural and behavioral data. My ongoing work explores how self-supervised and generative systems form hierarchical, multi-scale representations and decode behavior or speech from neural activity.</p>

<p><strong>Industry Experience</strong> — During my internships at <a href="https://research.facebook.com/">Meta</a>, I worked on developing multimodal foundation models for neural wristband and visual data fusion in 2024, advancing hand recognition and handwriting decoding models. In 2025, I joined the team to work on handwriting recognition using electromyography (EMG) signals from the <a href="https://www.meta.com/ai-glasses/meta-ray-ban-display/?srsltid=AfmBOoqJgguuzbd2lgoz1FicSW5Bnj5K1sXeNkIHcpDYIU3ILs_aoC2d">Meta Neural Band</a>, which capture electrical activity from subtle forearm muscle movements to enable intuitive, gesture-based control and text input. The project was <a href="https://www.youtube.com/live/D97ILdUbYww?si=krptKIikEhFvF9O9&t=2975">live demoed at Meta Connect 2025</a>, and the Meta Neural Band was later recognized as one of <a href="https://time.com/collections/best-inventions-2025/7318319/meta-ray-ban-display/"><em>TIME</em>’s Best Inventions of 2025</a>.</p>
<p class="about-text">
<span class="fas fa-file-alt about-icon"></span>
Explore my <a href="https://zhanqizhang66.github.io/publications">projects</a>, <a href="https://scholar.google.com/citations?user=NjLyahEAAAAJ&hl=en">publications</a>, and <a href="https://zhanqizhang66.github.io/Zhanqi%20Zhang%20CV_2024.pdf">CV</a>. My <a href="https://zhanqizhang66.github.io/blog/">blog</a> features essays and learning notes on AI, neuroscience, and more.
</p>

<p class="about-text">
<span class="fas fa-globe-americas about-icon"></span>
Grew up in <em>Jinan, China</em>; moved to <em>Beijing</em> during my teens; began undergrad in <em>St. Louis, MO</em> in 2016; now based in <em>San Diego, CA</em> for grad school.
</p>

<p class="about-text">
<span class="fas fa-heart about-icon"></span>
I love traveling, art, and animals! I live in a Disney dream with my free-flying birds: cockatiels Ashe and Pearl, and parakeet Kiwi. Check out my <a href="https://zhanqizhang66.github.io/art/">art portfolio</a>.
</p>


<!--  
</div>

<div id="research-section">
<h3>Graduate School Research</h3>
<ul>
  <! –– 
  <li>
    <p><a href="http://mishne.ucsd.edu/">Mishne Lab</a> and <a href="https://aoilab.biosci.ucsd.edu/">Aoi Lab</a> co-advised by Dr. Gal Mishne and Dr. Mikio Aoi</p>
    <p> Computer Science | University of California, San Diego</p>
    <p>This interdisciplinary research project aims to build a framework to answer questions in computational psychiatry. It uses probabilistic reasoning to quantify hallmark features related to bipolar disorder. By combining our methods with neural activity dynamics, we could further associate behaviors to neural recordings to gain more insights on how information processing maps onto behaviors, putting us one step closer to understanding the link between the brain and the mind.
    </p>
  </li>
</ul>

<h3>Undergrad Research</h3>
<ul>
  <li>
      <p><a href="https://ponce.hms.harvard.edu/">Ponce Lab</a> advised by Dr. Carlos Ponce</p>
    <p>Neurobiology | Harvard Medical School</p>
    <p>Automated visual recognition has the potential to change many facets of society, from biomedical imaging to security and transportation. Convolutional neural networks (CNNs) are the best models for visual recognition, and while they show enormous promise, they are notably vulnerable to "black-box attacks" -- malicious inputs designed to make the networks make mistakes. Because CNNs share many properties with the brain, we can understand what kind of attacks are particularly effective on the most resilient CNNs, by crafting attacks that manipulate activity in the brain. The Ponce lab has shown that it is possible to use generative adversarial networks (GANs) to maximize the activity of individual neurons in the brain, through the synthesis of artificial images. I explored which types of GANs are best in achieving this goal, in both macaque brains and convolutional neural networks.
      </p>
  </li>

  <li>
    <p><a href="http://www.clinicalpharmstl.org/research/Researcher%20Profile%20-%20Bhooma%20Aravamutha,%20M.D.,%20DPhil.html">Aravamuthan Lab</a> advised by Dr. Bhooma Aravamuthan</p> 
    <p>Neurology | Washington University in St. Louis School of Medicine</p>
    <p> I aimed to develop an open-field, video-based animal pose tracking framework using computer vision. Supervised machine learning tools analyze videos of animal behavior efficiently but are limited by operator’s labeling accuracy. To reduce operator dependency, I developed an unsupervised model based on optical flow to automatically detect and label mouse behaviors. I have shown that it is possible to differentiate between local movements (e.g. rearing), running, and stationary positions while matching human labeling accuracy. Additionally, I combined deep network-guided pose estimation (DeepLabCut) and optical flow to design a clinically feasible video-based dystonia identification tool.  </p>
  </li>
  <li>
    <p><a href="https://aimlab.wustl.edu/">AIM Lab</a> advised by Dr. Shantanu Chakrabartty</p>
    <p>Electrical Engineering | Washington University in St. Louis </p>
    <p>I investigated sonification techinique that can be used to visualize high-dimensional data like images. The goal is to understand the benefits of sonification compound to visual representations especially in the context of human-in-the-loop systems</p>
  </li>

</ul>


<h3>Teaching</h3> 
  <ul>
  <li>
      <p>Fall 2024, TA for <a href="https://omds.ucsd.edu/courses/dsc-257r-unsupervised-learning">DSC257R Unsupervised Learning</a> with <a href="https://cseweb.ucsd.edu/~dasgupta/">Sanjoy Dasgupta</a>. </p>
  </li> 
  <li>  
      <p>Fall 2020, Spring 2021, TA for <a href="https://courses.wustl.edu/CourseInfo.aspx?sch=E&dept=E81&crs=417T">CSE 417T Intro to Machine Learning</a></p> 
  </li>
  <li>
      <p>Fall 2019, TA for <a href="https://courses.wustl.edu/CourseInfo.aspx?sch=E&dept=E81&crs=247">CSE 247 Data Structures and Algorithms</a></p>
  </li>
  <li>
      <p>Fall 2018, Spring, Fall 2019, TA for</p><a href="https://courses.wustl.edu/CourseInfo.aspx?sch=E&dept=E35&crs=230">ESE 230 Intro to Electrical and Electronic Circuits</a></p>
  </li> 
  <li>
    <p>Fall 2018, Head TA for <a href="https://courses.wustl.edu/CourseInfo.aspx?sch=E&dept=E81&crs=240">CSE 240 Logic and Discrete Mathematics</a></p>
  </li>
  <li>
    <p><a></a></p>
  </li>
  </ul>
</div>
 -->

<div id="contactme-section">
<h3 id="contact">Contact</h3>
<p class="text-center mb-4">Feel free to reach out! You can email me at <a href="mailto:zhz091@ucsd.edu" class="text-primary">zhz091@ucsd.edu</a>, connect on <a href="https://twitter.com/ZhangZhanqi" class="text-primary">X</a>, or send a message using the form below:</p>

<form action="https://formspree.io/f/xayzdjey" method="POST" class="form shadow p-4 bg-white rounded" id="contact-form">
  <div class="row mb-3">
    <div class="col-md-6 mb-3 mb-md-0">
      <input type="email" name="_replyto" required="required" class="form-control form-control-lg" placeholder="Your Email" title="Email">
    </div>
    <div class="col-md-6">
      <input type="text" name="name" class="form-control form-control-lg" placeholder="Your Name" title="Name">
    </div>
  </div>
  <input type="hidden" name="_subject" value="You have a new message from ZhanqiZhang66.github.io">
  <div class="mb-3">
    <textarea type="text" name="content" class="form-control form-control-lg" placeholder="Your Message" title="Message" required="required" rows="4"></textarea>
  </div>
  <input type="text" name="_gotcha" style="display:none">
  <input type="hidden" name="_next" value="?message=Your message was sent successfully, thanks!" />
  
  <div class="text-center">
    <button type="submit" class="btn btn-lg btn-primary px-4">Send Message</button>
  </div>
</form>
</div>
