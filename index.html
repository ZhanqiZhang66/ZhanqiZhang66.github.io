---
layout: page
title: "Hi, I'm Victoria"
subtitle: "Researcher. Bird Lover." # in Artificial and Biological Intelligence
css: "/assets/css/index.css"
share-img: /assets/img/victoria_22.jpg
use-site-title: true
cover-img:
  - "/assets/img/bird/b_k_4.jpeg": "Blueberry and Kiwi"
---

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-Y06S3E3WTE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-Y06S3E3WTE');
</script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

### News
- My first-author submission, BEHAVE: Behavioral Ethology for Human Assessment via Variational Encoding, has been accepted for presentation at the NeurIPS 2025 Workshop on Data on Brain and Mind (DBM).
- My first-author paper, [Brain Feature Maps Reveal Progressive Animal-Feature Representations](https://www.science.org/doi/10.1126/sciadv.adq7342), is published on Science Advances, featured as "When Neurons Discover on Their Own" on [Harvard Brain Institute News](https://brain.harvard.edu/hbi_news/when-neurons-discover-on-their-own/).
- Check out my work on [Behavioral Dynamics in Bipolar Disorder](https://www.medrxiv.org/content/10.1101/2024.11.14.24317348v2) on medRxiv.
- Co-authored paper [Arousal as a universal embedding for spatiotemporal brain dynamics](https://www.nature.com/articles/s41586-025-09544-4) now on Nature.

### About me

{: .about-text}
<span class="fa fa-graduation-cap about-icon"></span>
Hi! My name is Victoria Zhanqi Zhang, 张展旗. I am a Ph.D. student in Computer Science at UCSD, supported by HDSI Phd Fellowship. I am  co-advised by [Dr. Mikio Aoi](https://aoilab.biosci.ucsd.edu/) and [Dr. Gal Mishne](http://mishne.ucsd.edu/). Previously, I studied Computer Science and Electrical Engineering at Washington University in St. Louis. I was advised by [Dr. Carlos Ponce](https://ponce.hms.harvard.edu/) as an undergrad research student.

{: .about-text}
<span class="fa fa-code about-icon"></span>
I build machine learning models to extract structure and understand dynamics in complex behavioral and brain systems. My research bridges computational neuroscience and deep learning to uncover how natural learning principles in the brain can inspire the development of more robust, generalizable, and interpretable AI systems.

My current and past projects span across:

- **Computational Neuroscience** — I study how neural representations and learning dynamics emerge across different levels of brain organization. My work combines electrophysiological decoding, hierarchical modeling, and behavioral analysis to uncover computational signatures of cognition and mental health. In my paper published in [*Science Advances*](https://www.science.org/doi/10.1126/sciadv.adq7342), *Brain Feature Maps Reveal Progressive Animal-Feature Representations*, I demonstrated how hierarchical representations of animal features emerge along the primate ventral visual stream, revealing how deep feature maps can model the progression of visual complexity in biological neural systems. I also develop data-driven models of human and animal behavior to identify state-dependent dynamics underlying psychiatric disorders such as bipolar disorder in my [pre-print](https://www.medrxiv.org/content/10.1101/2024.11.14.24317348v1).

- **Machine Learning** — I design and analyze machine learning models that learn interpretable and robust representations from complex neural and behavioral data. This includes ongoing work on decoding speech from brain activity using contrastive and generative (LLM-based) approaches, and on studying how latent hierarchies emerge in self-supervised systems to capture multi-scale organization of representations. During my internships at [Meta Reality Labs](https://about.meta.com/realitylabs/), I worked on developing multimodal foundation models for neural wristband and visual data fusion in 2024, advancing hand recognition and handwriting decoding models. In 2025, I joined the team to work on handwriting recognition using electromyography (EMG) signals from the [Meta Neuro Band](https://www.meta.com/ai-glasses/meta-ray-ban-display/?srsltid=AfmBOoqJgguuzbd2lgoz1FicSW5Bnj5K1sXeNkIHcpDYIU3ILs_aoC2d), which capture electrical activity from subtle forearm muscle movements to enable intuitive, gesture-based control and text input. The project was [live demoed at Meta Connect 2025](https://www.youtube.com/live/D97ILdUbYww?si=krptKIikEhFvF9O9&t=2975), and the Meta Neuro Band was later recognized as one of [*TIME*’s Best Inventions of 2025](https://time.com/collections/best-inventions-2025/7318319/meta-ray-ban-display/).


{: .about-text}
<span class="fa fa-file-alt about-icon"></span>
Explore my [projects](https://zhanqizhang66.github.io/publications), [publications](https://scholar.google.com/citations?user=NjLyahEAAAAJ&hl=en), and [CV](https://zhanqizhang66.github.io/Zhanqi%20Zhang%20CV_2024.pdf). My blog features essays and learning notes on AI, neuroscience, and more: [visit here](https://zhanqizhang66.github.io/blog/).

{: .about-text}
<span class="fa fa-globe-americas about-icon"></span>
Grew up in *Jinan, China*; moved to *Beijing, China* during my teens; Began my undergrad in *St. Louis, MO* in 2016; and now live in *San Diego, CA* for grad school;

{: .about-text}
<span class="fa fa-heart about-icon"></span>
I enjoy traveling and art. I live with happy free-flying birds: cockatiel Ashe, Pearl, and parakeet Kiwi. I DIY swings and castles for them and teach them songs and cool tricks. Check out my [art portfolio](https://zhanqizhang66.github.io/art/).

<!--  
</div>

<div id="research-section">
<h3>Graduate School Research</h3>
<ul>
  <! –– 
  <li>
    <p><a href="http://mishne.ucsd.edu/">Mishne Lab</a> and <a href="https://aoilab.biosci.ucsd.edu/">Aoi Lab</a> co-advised by Dr. Gal Mishne and Dr. Mikio Aoi</p>
    <p> Computer Science | University of California, San Diego</p>
    <p>This interdisciplinary research project aims to build a framework to answer questions in computational psychiatry. It uses probabilistic reasoning to quantify hallmark features related to bipolar disorder. By combining our methods with neural activity dynamics, we could further associate behaviors to neural recordings to gain more insights on how information processing maps onto behaviors, putting us one step closer to understanding the link between the brain and the mind.
    </p>
  </li>
</ul>

<h3>Undergrad Research</h3>
<ul>
  <li>
      <p><a href="https://ponce.hms.harvard.edu/">Ponce Lab</a> advised by Dr. Carlos Ponce</p>
    <p>Neurobiology | Harvard Medical School</p>
    <p>Automated visual recognition has the potential to change many facets of society, from biomedical imaging to security and transportation. Convolutional neural networks (CNNs) are the best models for visual recognition, and while they show enormous promise, they are notably vulnerable to "black-box attacks" -- malicious inputs designed to make the networks make mistakes. Because CNNs share many properties with the brain, we can understand what kind of attacks are particularly effective on the most resilient CNNs, by crafting attacks that manipulate activity in the brain. The Ponce lab has shown that it is possible to use generative adversarial networks (GANs) to maximize the activity of individual neurons in the brain, through the synthesis of artificial images. I explored which types of GANs are best in achieving this goal, in both macaque brains and convolutional neural networks.
      </p>
  </li>

  <li>
    <p><a href="http://www.clinicalpharmstl.org/research/Researcher%20Profile%20-%20Bhooma%20Aravamutha,%20M.D.,%20DPhil.html">Aravamuthan Lab</a> advised by Dr. Bhooma Aravamuthan</p> 
    <p>Neurology | Washington University in St. Louis School of Medicine</p>
    <p> I aimed to develop an open-field, video-based animal pose tracking framework using computer vision. Supervised machine learning tools analyze videos of animal behavior efficiently but are limited by operator’s labeling accuracy. To reduce operator dependency, I developed an unsupervised model based on optical flow to automatically detect and label mouse behaviors. I have shown that it is possible to differentiate between local movements (e.g. rearing), running, and stationary positions while matching human labeling accuracy. Additionally, I combined deep network-guided pose estimation (DeepLabCut) and optical flow to design a clinically feasible video-based dystonia identification tool.  </p>
  </li>
  <li>
    <p><a href="https://aimlab.wustl.edu/">AIM Lab</a> advised by Dr. Shantanu Chakrabartty</p>
    <p>Electrical Engineering | Washington University in St. Louis </p>
    <p>I investigated sonification techinique that can be used to visualize high-dimensional data like images. The goal is to understand the benefits of sonification compound to visual representations especially in the context of human-in-the-loop systems</p>
  </li>

</ul>


<h3>Teaching</h3> 
  <ul>
  <li>
      <p>Fall 2024, TA for <a href="https://omds.ucsd.edu/courses/dsc-257r-unsupervised-learning">DSC257R Unsupervised Learning</a> with <a href="https://cseweb.ucsd.edu/~dasgupta/">Sanjoy Dasgupta</a>. </p>
  </li> 
  <li>  
      <p>Fall 2020, Spring 2021, TA for <a href="https://courses.wustl.edu/CourseInfo.aspx?sch=E&dept=E81&crs=417T">CSE 417T Intro to Machine Learning</a></p> 
  </li>
  <li>
      <p>Fall 2019, TA for <a href="https://courses.wustl.edu/CourseInfo.aspx?sch=E&dept=E81&crs=247">CSE 247 Data Structures and Algorithms</a></p>
  </li>
  <li>
      <p>Fall 2018, Spring, Fall 2019, TA for</p><a href="https://courses.wustl.edu/CourseInfo.aspx?sch=E&dept=E35&crs=230">ESE 230 Intro to Electrical and Electronic Circuits</a></p>
  </li> 
  <li>
    <p>Fall 2018, Head TA for <a href="https://courses.wustl.edu/CourseInfo.aspx?sch=E&dept=E81&crs=240">CSE 240 Logic and Discrete Mathematics</a></p>
  </li>
  <li>
    <p><a></a></p>
  </li>
  </ul>
</div>
 -->

<div id="contactme-section">
<h3 id="contact">Contact</h3>
<p>Email me at <a href="mailto:zhz091@ucsd.edu">zhz091@ucsd.edu</a> or reach out via <a href="https://twitter.com/ZhangZhanqi">X</a> or send a message below:</p>

<form action="https://formspree.io/f/xayzdjey" method="POST" class="form" id="contact-form">
  <div class="row">
    <div class="col-6">
      <input type="email" name="_replyto" required="required" class="form-control form-control-lg" placeholder="Email" title="Email">
    </div>
    <div class="col-6">
      <input type="text" name="name" class="form-control form-control-lg" placeholder="Name" title="Name">
    </div>
  </div>
  <input type="hidden" name="_subject" value="You have a new message from ZhanqiZhang66.github.io">
  <textarea type="text" name="content" class="form-control form-control-lg" placeholder="Message" title="Message" required="required" rows="3"></textarea>
  <input type="text" name="_gotcha" style="display:none">
  <input type="hidden" name="_next" value="?message=Your message was sent successfully, thanks!" />
  
  <button type="submit" class="btn btn-lg btn-primary">Submit</button>
</form>
</div>
