---
layout: post
title: But, what is Attention in transformers?
subtitle: Q,K,V, with code snippets

cover-img: /assets/img/photograph/19AF8CFD-3763-4B3F-8735-AA01887F3315.jpg
thumbnail-img: /assets/img/thumb.png
share-img: /assets/img/photograph/19AF8CFD-3763-4B3F-8735-AA01887F3315.jpg
tags: [learning notes]
---


<h2>‘All’ you need to know about transformer</h2>

Attention is well used in modern transformer learning models. Okay, but what is attention exactly? I often find myself going back to the great work "Attention is all you need" over and over again when I need to explain the concept to others and to myself.

As an engineer, I found it useful to understand math through code, so inspired by [Jacob Gildenblat's post on explainability of vision transformers](https://jacobgil.github.io/deeplearning/vision-transformer-explainability), I made this attention learning note with code snippets in PyTorch.

For Q, K, V, attention, and transformer model, there is visual illustration along with detailed code snippets for each concept/math equation.

Reference other the papers cited in the pdf:
+ [Vision Transformer Tutorial Google Colab](https://colab.research.google.com/github/hirotomusiker/schwert_colab_data_storage/blob/master/notebook/Vision_Transformer_Tutorial.ipynb)
+ [Illustrated self attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)
+ [Jacob Gildenblat's post on explainability of vision transformers](https://jacobgil.github.io/deeplearning/vision-transformer-explainability)

<iframe width="100%" height="800" src="/files/Exploring Explainability for Vision Transformers.pdf">
